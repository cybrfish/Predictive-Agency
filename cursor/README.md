# Predictive Agency - Core Engine

**An interactive demo proving that AI systems can be designed for regenerative behavior through average-reward reinforcement learning.**

## Quick Start

```bash
npm install
npm run dev
# Open http://localhost:5173
```

## What Is This?

This is **Phase 0** of Predictive Agency: validating the core math before building fancy visualizations.

You'll see:
- 200 agents with differential Q-learning
- Average reward (rÃÑ) converging over time
- The **Beaver Test** in action - boundary expansion revealing extractive behavior
- Two scenarios: Token Rideshare (fails) vs Open Rails (passes)

## The Core Idea

**Most systems optimize locally** (maximize my reward now).  
**Predictive Agency optimizes systemically** (maximize predicted long-term system health).

The math enforces this automatically through **average-reward RL**:
```
r* = lim(n‚Üí‚àû) (1/n) Œ£ E[R_t]
```

Agents that extract (high œÑ) eventually tank r* and get suppressed by the controller. No moral lecturing needed - just optimization.

## The Beaver Test

Draw a boundary around a process. Measure usable energy (r*) inside.  
Now **expand the boundary** to include the wider system.

- **Pass (Beaver):** r* increases at every scale
- **Fail (Algae):** Looks good narrowly, disaster widely

**Alignment coefficient:**
```
Œ± = r*_ecosystem / r*_network

> 1: regenerative
‚âà 1: neutral
< 1: extractive
< 0: destructive
```

## Project Structure

```
src/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ types.ts              # Core interfaces
‚îÇ   ‚îú‚îÄ‚îÄ Agent.ts              # Agent state machine
‚îÇ   ‚îú‚îÄ‚îÄ DifferentialQ.ts      # Q-learning controller
‚îÇ   ‚îú‚îÄ‚îÄ BoundaryManager.ts    # Boundary expansion logic
‚îÇ   ‚îú‚îÄ‚îÄ PowerCalculator.ts    # Monte Carlo Shapley
‚îÇ   ‚îî‚îÄ‚îÄ Simulation.ts         # Main loop
‚îú‚îÄ‚îÄ scenarios/
‚îÇ   ‚îî‚îÄ‚îÄ presets.ts            # Token vs Open Rails configs
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ Plot.svelte           # D3 line charts
‚îÇ   ‚îî‚îÄ‚îÄ Metrics.svelte        # Readouts
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ DifferentialQ.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ BoundaryTest.test.ts
‚îÇ   ‚îî‚îÄ‚îÄ PowerCalc.test.ts
‚îî‚îÄ‚îÄ App.svelte                # Main UI
```

## Implementation Status

**Phase 0: Core Engine (IN PROGRESS)**
- [x] Type definitions
- [x] Scenario presets
- [ ] Agent implementation
- [ ] Differential Q-learning
- [ ] Boundary manager
- [ ] Power calculator
- [ ] Simulation loop
- [ ] Minimal UI
- [ ] Unit tests

## Key Equations

**Differential Q-Learning:**
```
Œ¥_t = R_t - rÃÑ + max_a' Q(s',a') - Q(s,a)
Q ‚Üê Q + Œ± Œ¥_t
rÃÑ ‚Üê rÃÑ + Œ≤ Œ¥_t
```

**Reward Function:**
```
R_t = w^T x_t - Œª_t - Œ∫_œÅ œÅ_t - Œ∫_œÉ œÉ_t
where x_t = [demand, capacity, safety, surplus, trust, congestion]
```

## Expected Behavior

### Token Rideshare (œÑ=25%)
- rÃÑ converges to ~40-50
- Œ± > 1 at B_0 (looks good locally)
- Œ± < 1 at B_1 (extractive once externalities included) **‚Üê BEAVER TEST FAIL**

### Open Rails (œÑ=2%)
- rÃÑ converges to ~70-80
- Œ± > 1 at B_0, B_1, B_2 (regenerative at all scales) **‚Üê BEAVER TEST PASS**

## Documentation

- **Full spec:** `/home/claude/IMPLEMENTATION_SPEC.md`
- **Framework:** `/home/claude/predictive-agency-framework.md`
- **PRD:** `/home/claude/predictive-agency-prd.md`

---

**"Predictive Agency: Agents that see around corners."** ü¶´
