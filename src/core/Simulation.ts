import { Agent } from './Agent';
import type { GlobalState, HistoryEntry } from './types';
import { DEFAULT_CONFIG } from './types';
import { DifferentialQLearning } from './DifferentialQ';
import type { ScenarioConfig } from './types';
import { BoundaryManager } from './BoundaryManager';
import { PowerCalculator } from './PowerCalculator';

export class Simulation {
  agents: Agent[] = [];
  globalState: GlobalState;
  qLearning: DifferentialQLearning;
  boundary: BoundaryManager;
  powerCalc: PowerCalculator;
  t = 0;
  history: HistoryEntry[] = [];
  config = DEFAULT_CONFIG;
  
  constructor(scenario: ScenarioConfig) {
    // Initialize agents from scenario
    let idCounter = 0;
    for (const group of scenario.agentConfig) {
      for (let i = 0; i < group.count; i++) {
        this.agents.push(new Agent(idCounter++, group.type, group.takeRate, group.serviceLevel));
      }
    }
    
    // Initialize state
    this.globalState = {
      demand: 60,
      capacity: 50,
      safety: 70,
      surplus: 55,
      trust: 60,
      congestion: 25,
    };
    
    this.qLearning = new DifferentialQLearning(this.config.alphaQ, this.config.betaR);
    this.boundary = new BoundaryManager();
    this.powerCalc = new PowerCalculator();
  }
  
  step(): void {
    // 1. Observe and select actions
    const possibleActions = this.qLearning.getPossibleActions();
    for (const agent of this.agents) {
      agent.observe(this.globalState);
      agent.action = agent.selectAction(possibleActions);
    }
    
    // 2. Update global state based on average actions
    this.updateGlobalState();
    
    // 3. Compute reward
    const reward = this.computeReward();
    
    // 4. Update Q for each agent
    for (const agent of this.agents) {
      this.qLearning.update(agent, this.globalState, agent.action, reward, this.globalState);
    }
    
    // 5. Compute metrics every 10 steps
    if (this.t % 10 === 0) {
      for (const agent of this.agents) {
        agent.contribution = agent.computeContribution(this.globalState);
      }
      for (const agent of this.agents) {
        agent.power = this.powerCalc.computePower(
          agent, this.agents, this.qLearning.rBar
        );
      }
    }
    
    // 6. Record history
    this.history.push({
      t: this.t,
      rBar: this.qLearning.rBar,
      reward: reward,
      alpha: this.boundary.computeAlignmentCoefficient(
        this.computeEcosystemReward(),
        this.computeNetworkReward()
      ),
      state: { ...this.globalState },
    });
    
    this.t++;
  }
  
  private updateGlobalState(): void {
    const avgTakeRate = this.agents.reduce((s, a) => s + a.action.takeRate, 0) / this.agents.length;
    const avgService = this.agents.reduce((s, a) => s + a.action.serviceLevel, 0) / this.agents.length;

    // Revised dynamics with more entropy and feedback
    const s = this.globalState;

    // Surplus: Generated by service, drained by take rate and system upkeep.
    s.surplus = clamp(s.surplus + (avgService * 2.5) - (avgTakeRate * 12) - 0.3, 0, 100);

    // Trust: Grows with service, decays with extraction, congestion, and time.
    s.trust = clamp(s.trust + (avgService - avgTakeRate) * 1.5 - (s.congestion / 100) * 2 - 0.2, 0, 100);

    // Congestion: Rises with take rate (less reinvestment) and system strain, but dissipates slowly.
    s.congestion = clamp(s.congestion + (avgTakeRate * 5) + (s.demand / (s.capacity + 1)) * 1 - 0.5, 0, 100);

    // Safety: Increases with high service, but is eroded by congestion and extractive policies.
    s.safety = clamp(s.safety + (avgService * 1.0) - (s.congestion * 0.05) - (avgTakeRate * 3) - 0.1, 0, 100);

    // Capacity: Requires service investment to grow, decays slowly over time.
    s.capacity = clamp(s.capacity + (avgService * 1.2) - (s.congestion * 0.03) - 0.15, 0, 100);

    // Demand: Responds to how well the system is meeting needs (surplus).
    s.demand = clamp(s.demand + (50 - s.surplus) * 0.02, 20, 80);
  }
  
  private computeReward(): number {
    const s = this.globalState;
    const weights = this.boundary.getRewardWeights();
    const value = 0.3 * s.surplus + 0.25 * s.safety + 0.2 * s.trust + 0.15 * s.capacity + 0.1 * s.demand;
    const avgExtraction = this.agents.reduce((sum, a) => sum + a.action.takeRate * 10, 0) / this.agents.length;
    const friction = 0.4 * s.congestion + 0.6 * (100 - s.safety);
    return value - avgExtraction - friction * weights.friction;
  }

  computeEcosystemReward(): number {
    if (this.history.length < 50) return this.qLearning.rBar;
    // r* at current boundary - average of last 50 rewards
    return this.history.slice(-50).reduce((sum, h) => sum + h.reward, 0) / 50;
  }
  
  computeNetworkReward(): number {
    // Narrow view of network health (surplus + trust, ignoring externalities)
    return (this.globalState.surplus * 0.7 + this.globalState.trust * 0.3) / 2;
  }
}

function clamp(v: number, lo: number, hi: number): number {
  return Math.max(lo, Math.min(hi, v));
}


